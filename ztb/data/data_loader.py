# ãƒ‡ãƒ¼ã‚¿å“è³ªè©³ç´°ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# å¤–ã‚Œå€¤æ¤œå‡ºã¨è©³ç´°ãªåˆ†å¸ƒåˆ†æ

import json
import logging
import os
from pathlib import Path
from typing import Union

import numpy as np
import numpy.ma as ma
import pandas as pd
from scipy import stats

from ztb.utils.data.outlier_detection import (
    detect_outliers_iqr,
    detect_outliers_zscore,
)
from ztb.utils.errors import safe_operation

logger = logging.getLogger(__name__)

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
# sys.path.append(str(Path(__file__).parent.parent.parent))


def detect_outliers_iqr(
    data: pd.DataFrame, column: str
) -> tuple[pd.DataFrame, float, float]:
    """IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º"""
    from ztb.utils.data.outlier_detection import detect_outliers_iqr as _detect_outliers_iqr
    return _detect_outliers_iqr(data, column)


def detect_outliers_zscore(
    data: pd.DataFrame, column: str, threshold: float = 3
) -> pd.DataFrame:
    """Z-scoreæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º"""
    from ztb.utils.data.outlier_detection import detect_outliers_zscore as _detect_outliers_zscore
    return _detect_outliers_zscore(data, column, threshold)


def get_project_root() -> Path:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®ãƒ‘ã‚¹ã‚’å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•°ã‚„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ä¸Šæ›¸ãå¯èƒ½ï¼‰"""
    return safe_operation(
        logger,
        _get_project_root_impl,
        "get_project_root",
        Path(__file__).parent.parent.parent  # Default fallback
    )


def _get_project_root_impl() -> Path:
    """Implementation of getting project root."""
    # ç’°å¢ƒå¤‰æ•°ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†
    env_root = os.environ.get("PROJECT_ROOT")
    if env_root:
        return Path(env_root).resolve()
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆconfig/rl_config.jsonï¼‰ã«project_rootã‚­ãƒ¼ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†
    config_path = Path(__file__).parent.parent.parent / "config" / "rl_config.json"
    if config_path.exists():
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
            project_root = config.get("project_root")
            if project_root:
                return Path(project_root).resolve()
        except Exception:
            pass
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return Path(__file__).parent.parent.parent


def project_path(*parts: str) -> Path:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ„ã¿ç«‹ã¦ã‚‹"""
    return get_project_root().joinpath(*parts)


def analyze_feature_distributions(
    multiplier: float = 1.0, config_path: Union[str, Path, None] = None
) -> None:
    """ç‰¹å¾´é‡åˆ†å¸ƒã®è©³ç´°åˆ†æ"""
    print("=== è©³ç´°ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ ===")

    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    env_config_path = os.environ.get("RL_CONFIG_PATH")
    if config_path is None:
        config_path = env_config_path or project_path("config", "rl_config.json")

    config_path = Path(config_path)
    if config_path.exists():
        with open(config_path, "r") as f:
            config = json.load(f)
        base_threshold = (
            config.get("data_quality", {}).get("outlier_threshold_percent", 10.0)
            / 100.0
        )
    else:
        base_threshold = 0.10  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10%

    outlier_threshold = base_threshold * multiplier
    print(f"å¤–ã‚Œå€¤é–¾å€¤: Â±{outlier_threshold * 100}% (multiplier: {multiplier})")
    data_paths = [
        project_path("data", "features", "2025", "04", "sample_04.parquet"),
        project_path("data", "features", "2025", "05", "sample_05.parquet"),
        project_path("data", "features", "2025", "06", "sample_06.parquet"),
    ]

    all_data = []
    for path in data_paths:
        if path.exists():
            df = pd.read_parquet(path)
            all_data.append(df)

    if not all_data:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    combined_df = pd.concat(all_data, ignore_index=True)
    print(f"çµåˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {combined_df.shape}")
    print(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {combined_df['ts'].min()} ã‹ã‚‰ {combined_df['ts'].max()}")

    # æ¬ æå€¤ã®è©³ç´°ãƒã‚§ãƒƒã‚¯
    print("\n=== æ¬ æå€¤åˆ†æ ===")
    null_counts = combined_df.isnull().sum()
    total_nulls = null_counts.sum()
    print(f"ç·æ¬ æå€¤: {total_nulls}")

    if total_nulls > 0:
        print("æ¬ æå€¤ã®ã‚ã‚‹åˆ—:")
        price_changes = combined_df["price"].pct_change().shift(-1)
        extreme_changes = combined_df[price_changes.abs() > outlier_threshold]
        print(f"ä¾¡æ ¼å¤‰å‹• Â±{outlier_threshold * 100}% è¶…: {len(extreme_changes)} ä»¶")
        if len(extreme_changes) > 0:
            print("æ¥µç«¯å¤‰å‹•ã‚µãƒ³ãƒ—ãƒ«:")
            for idx, _ in extreme_changes.head(3).iterrows():
                change_pct = price_changes.at[idx] * 100
                print(f"  Index {idx}: å¤‰å‹•ç‡ {change_pct:.2f}%")
    print("\n=== å¤–ã‚Œå€¤æ¤œå‡º ===")
    numeric_cols = combined_df.select_dtypes(include=[np.number]).columns
    key_features = ["price", "volume", "sma_5", "sma_10", "rsi_14", "macd"]

    outlier_summary = []

    # ä¾¡æ ¼å¤‰å‹•ã®æ¥µç«¯å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
    if "price" in combined_df.columns:
        price_changes = combined_df["price"].pct_change()
        extreme_changes = combined_df[price_changes.abs() > outlier_threshold]
        print(f"ä¾¡æ ¼å¤‰å‹• Â±{outlier_threshold * 100}% è¶…: {len(extreme_changes)} ä»¶")
        if len(extreme_changes) > 0:
            print("æ¥µç«¯å¤‰å‹•ã‚µãƒ³ãƒ—ãƒ«:")
            for idx, _ in extreme_changes.head(3).iterrows():
                change_pct = price_changes.at[idx] * 100
                print(f"  Index {idx}: å¤‰å‹•ç‡ {change_pct:.2f}%")

    for col in key_features:
        if col in numeric_cols:
            # IQRæ³•
            outliers_iqr, lower, upper = detect_outliers_iqr(combined_df, col)
            # Z-scoreæ³•
            outliers_z = detect_outliers_zscore(combined_df, col)

            outlier_info = {
                "feature": col,
                "iqr_outliers": len(outliers_iqr),
                "zscore_outliers": len(outliers_z),
                "iqr_percentage": len(outliers_iqr) / len(combined_df) * 100,
                "zscore_percentage": len(outliers_z) / len(combined_df) * 100,
                "lower_bound": lower,
                "upper_bound": upper,
            }
            outlier_summary.append(outlier_info)

    # å¤–ã‚Œå€¤ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    print("ç‰¹å¾´é‡åˆ¥å¤–ã‚Œå€¤æ¤œå‡ºçµæœ:")
    print("-" * 80)
    print(
        f"{'Feature':<12} {'IQR Outliers':<20} {'Z-score Outliers':<22} {'IQR Range'}"
    )
    print("-" * 80)

    for info in outlier_summary:
        print(
            f"{info['feature']:<12} {info['iqr_outliers']} ({info['iqr_percentage']:.2f}%) "
            f"Z-score: {info['zscore_outliers']} ({info['zscore_percentage']:.2f}%) "
            f"ç¯„å›²: [{info['lower_bound']:.2f}, {info['upper_bound']:.2f}]"
        )

    # åˆ†å¸ƒã®è©³ç´°åˆ†æ
    print("\n=== åˆ†å¸ƒè©³ç´°åˆ†æ ===")
    for col in key_features[:3]:  # æœ€åˆã®3ã¤ã®ç‰¹å¾´é‡ã®ã¿è©³ç´°åˆ†æ
        if col in numeric_cols:
            data = combined_df[col].dropna()
            print(f"\n{col} ã®è©³ç´°çµ±è¨ˆ:")

            # åŸºæœ¬çµ±è¨ˆ
            desc = data.describe()
            print(f"  å¹³å‡: {desc['mean']:.2f}")
            print(f"  æ¨™æº–åå·®: {desc['std']:.2f}")
            print(f"  æœ€å°å€¤: {desc['min']:.2f}")
            print(f"  ç¬¬1å››åˆ†ä½æ•°: {desc['25%']:.2f}")
            print(f"  ä¸­å¤®å€¤: {desc['50%']:.2f}")
            print(f"  ç¬¬3å››åˆ†ä½æ•°: {desc['75%']:.2f}")
            print(f"  æœ€å¤§å€¤: {desc['max']:.2f}")

            # æ­ªåº¦ã¨å°–åº¦
            skewness = data.skew()
            kurtosis = data.kurtosis()
            print(f"  æ­ªåº¦: {skewness:.4f}")  # type: ignore[str-bytes-safe]
            print(f"  å°–åº¦: {kurtosis:.4f}")  # type: ignore[str-bytes-safe]

            # æ­£è¦æ€§æ¤œå®š
            if len(data) >= 3 and len(data) <= 5000:  # Shapiro-Wilkæ¤œå®šã®å…¬å¼æ¨å¥¨ç¯„å›²
                sample_size = len(data)
                _, p_value = stats.shapiro(
                    data.sample(sample_size, random_state=42, replace=False)
                )
                print(
                    f"  æ­£è¦æ€§æ¤œå®š på€¤: {p_value:.6f} ({'æ­£è¦åˆ†å¸ƒ' if p_value > 0.05 else 'éæ­£è¦åˆ†å¸ƒ'})"
                )

    print("\n=== ãƒ‡ãƒ¼ã‚¿é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯ ===")
    # tsåˆ—ãŒdatetimeå‹ã§ãªã„å ´åˆã®ã¿å¤‰æ›ï¼ˆunitæŒ‡å®šã¯intå‹ã®ã¨ãã®ã¿ï¼‰
    if not pd.api.types.is_datetime64_any_dtype(combined_df["ts"]):
        try:
            # intå‹ãªã‚‰unit='s'ã§å¤‰æ›ã€ãã†ã§ãªã‘ã‚Œã°unitãªã—
            if pd.api.types.is_integer_dtype(combined_df["ts"]):
                combined_df["ts"] = pd.to_datetime(combined_df["ts"], unit="s")
            else:
                combined_df["ts"] = pd.to_datetime(combined_df["ts"])
        except Exception as e:
            print(f"âŒ tsåˆ—ã®datetimeå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return  # å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã¯ä»¥é™ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—

    if pd.api.types.is_datetime64_any_dtype(combined_df["ts"]):
        time_diffs = combined_df["ts"].sort_values().diff()
        print("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é–“éš”ã®çµ±è¨ˆ:")
        print(time_diffs.describe())
        print(
            f"  æœ€é »é–“éš”: {time_diffs.mode().iloc[0] if not time_diffs.mode().empty else 'N/A'}"
        )
        print(f"  æœ€å¤§é–“éš”: {time_diffs.max()}")
        print(f"  æ¬ æé–“éš”æ•° (5åˆ†ä»¥ä¸Š): {(time_diffs > pd.Timedelta('5min')).sum()}")
    else:
        print("tsåˆ—ãŒdatetimeå‹ã§ãªã„ãŸã‚ã€é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    # ç‰¹å¾´é‡ã®ç›¸é–¢ãƒã‚§ãƒƒã‚¯
    print("\n=== ç‰¹å¾´é‡ç›¸é–¢ãƒã‚§ãƒƒã‚¯ ===")
    # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ã§ç›¸é–¢ã‚’è¨ˆç®—
    available_features = [col for col in key_features if col in combined_df.columns]
    if available_features:
        correlation_matrix = combined_df[available_features].corr()
        print("ä¸»è¦ç‰¹å¾´é‡ã®ç›¸é–¢ä¿‚æ•°:")
        print(correlation_matrix.round(3))

        # é«˜ç›¸é–¢ã®ãƒšã‚¢ã‚’ç‰¹å®š
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i + 1, len(correlation_matrix.columns)):
                corr = correlation_matrix.iloc[i, j]
                if isinstance(corr, (int, float)) and abs(corr) > 0.8:
                    high_corr_pairs.append(
                        (
                            correlation_matrix.columns[i],
                            correlation_matrix.columns[j],
                            corr,
                        )
                    )

        if high_corr_pairs:
            print("\né«˜ç›¸é–¢ã®ç‰¹å¾´é‡ãƒšã‚¢ (ç›¸é–¢ä¿‚æ•° > 0.8):")
            for feat1, feat2, corr in high_corr_pairs:
                print(f"{feat1} - {feat2}: ç›¸é–¢ä¿‚æ•° {corr:.3f}")
        else:
            print("\né«˜ç›¸é–¢ã®ç‰¹å¾´é‡ãƒšã‚¢ãªã—")
    else:
        print("ä¸»è¦ç‰¹å¾´é‡ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ” è©³ç´°ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹")

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§multiplierã¨config_pathã‚’å–å¾—
    import argparse

    parser = argparse.ArgumentParser(description="è©³ç´°ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument(
        "--multiplier",
        type=float,
        default=1.0,
        help="å¤–ã‚Œå€¤æ¤œå‡ºã®é–¾å€¤å€ç‡ï¼ˆä¾‹: 1.5ã«ã™ã‚‹ã¨é–¾å€¤ãŒ1.5å€ã«ãªã‚Šã¾ã™ï¼‰ã€‚ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚„ç•°å¸¸å€¤ã®è¨±å®¹åº¦ã«å¿œã˜ã¦èª¿æ•´ã—ã¦ãã ã•ã„ã€‚",
    )
    parser.add_argument(
        "--config_path",
        type=str,
        default=None,
        help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONå½¢å¼ï¼‰ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¾ã™ã€‚æœªæŒ‡å®šã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°RL_CONFIG_PATHã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚",
    )
    args = parser.parse_args()

    try:
        analyze_feature_distributions(
            multiplier=args.multiplier, config_path=args.config_path
        )
        print("\n" + "=" * 60)
        print("âœ… è©³ç´°ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†")

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback

        traceback.print_exc()
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
        error_log_path = project_path("data_quality_error.log")
        with open(error_log_path, "a", encoding="utf-8") as log_file:
            log_file.write(f"ã‚¨ãƒ©ãƒ¼: {e}\n")
            traceback.print_exc(file=log_file)
        # ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã§çµ‚äº†
        import sys

        sys.exit(1)


if __name__ == "__main__":
    main()
