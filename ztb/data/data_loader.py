# ãƒ‡ãƒ¼ã‚¿å“è³ªè©³ç´°ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# å¤–ã‚Œå€¤æ¤œå‡ºã¨è©³ç´°ãªåˆ†å¸ƒåˆ†æ

import sys
import os
import pandas as pd
import numpy as np
import numpy.ma as ma
from pathlib import Path
from scipy import stats
import json
from typing import Union

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
# sys.path.append(str(Path(__file__).parent.parent.parent))

def detect_outliers_iqr(data, column):
    """IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º"""
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

def detect_outliers_zscore(data, column, threshold=3):
    """Z-scoreæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º"""
    series = data[column]
    # stats.zscoreã¯MaskedArrayã‚’è¿”ã™ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€é€šå¸¸ã®numpyé…åˆ—ã«å¤‰æ›
    z_scores_raw = stats.zscore(series, nan_policy='omit')
    
    # MaskedArrayã‚’numpyé…åˆ—ã«å¤‰æ›ã—ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸå€¤ã‚’NaNã§åŸ‹ã‚ã‚‹
    # np.ma.filledã¯MaskedArrayã¨é€šå¸¸ã®ndarrayã®ä¸¡æ–¹ã‚’å‡¦ç†ã§ãã‚‹
    z_scores_unmasked = ma.filled(z_scores_raw, np.nan)
    
    # NaNã‚’ãã®ã¾ã¾é™¤å¤–ï¼ˆ0ã«å¤‰æ›ã—ãªã„ï¼‰
    z_scores_series = pd.Series(np.abs(z_scores_unmasked), index=series.index)
    outliers = data[z_scores_series > threshold]
    return outliers

def analyze_feature_distributions(multiplier: float = 1.0, config_path: Union[str, Path, None] = None):
    """ç‰¹å¾´é‡åˆ†å¸ƒã®è©³ç´°åˆ†æ"""
    print("=== è©³ç´°ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ ===")

    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    env_config_path = os.environ.get("RL_CONFIG_PATH")
    if config_path is None:
        config_path = env_config_path or Path(__file__).parent.parent.parent / "config/rl_config.json"
    
    config_path = Path(config_path)
    if config_path.exists():
        with open(config_path, 'r') as f:
            config = json.load(f)
        base_threshold = config.get('data_quality', {}).get('outlier_threshold_percent', 10.0) / 100.0
    else:
        base_threshold = 0.10  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10%

    outlier_threshold = base_threshold * multiplier
    print(f"å¤–ã‚Œå€¤é–¾å€¤: Â±{outlier_threshold * 100}% (multiplier: {multiplier})")
    data_paths = [
        Path(__file__).parent.parent.parent / "data/features/2025/04/sample_04.parquet",
        Path(__file__).parent.parent.parent / "data/features/2025/05/sample_05.parquet",
        Path(__file__).parent.parent.parent / "data/features/2025/06/sample_06.parquet",
    ]

    all_data = []
    for path in data_paths:
        if path.exists():
            df = pd.read_parquet(path)
            all_data.append(df)

    if not all_data:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    combined_df = pd.concat(all_data, ignore_index=True)
    print(f"çµåˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {combined_df.shape}")
    print(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {combined_df['ts'].min()} ã‹ã‚‰ {combined_df['ts'].max()}")

    # æ¬ æå€¤ã®è©³ç´°ãƒã‚§ãƒƒã‚¯
    print("\n=== æ¬ æå€¤åˆ†æ ===")
    null_counts = combined_df.isnull().sum()
    total_nulls = null_counts.sum()
    print(f"ç·æ¬ æå€¤: {total_nulls}")

    if total_nulls > 0:
        print("æ¬ æå€¤ã®ã‚ã‚‹åˆ—:")
        non_zero_nulls = null_counts[null_counts > 0]
        for col, count in non_zero_nulls.items():
            null_ratio = count / len(combined_df) * 100
            print(f"{col}: {count}ä»¶ ({null_ratio:.2f}%)")
    else:
        print("âœ… æ¬ æå€¤ãªã— - ãƒ‡ãƒ¼ã‚¿å“è³ªè‰¯å¥½")

    # å¤–ã‚Œå€¤æ¤œå‡º
    print("\n=== å¤–ã‚Œå€¤æ¤œå‡º ===")
    numeric_cols = combined_df.select_dtypes(include=[np.number]).columns
    key_features = ['price', 'volume', 'sma_5', 'sma_10', 'rsi_14', 'macd']

    outlier_summary = []

    # ä¾¡æ ¼å¤‰å‹•ã®æ¥µç«¯å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
    if 'price' in combined_df.columns:
        price_changes = combined_df['price'].pct_change()
        extreme_changes = combined_df[price_changes.abs() > outlier_threshold]
        print(f"ä¾¡æ ¼å¤‰å‹• Â±{outlier_threshold * 100}% è¶…: {len(extreme_changes)} ä»¶")
        if len(extreme_changes) > 0:
            print("æ¥µç«¯å¤‰å‹•ã‚µãƒ³ãƒ—ãƒ«:")
            for idx, _ in extreme_changes.head(3).iterrows():
                change_pct = price_changes.at[idx] * 100
                print(f"  Index {idx}: å¤‰å‹•ç‡ {change_pct:.2f}%")

    for col in key_features:
        if col in numeric_cols:
            # IQRæ³•
            outliers_iqr, lower, upper = detect_outliers_iqr(combined_df, col)
            # Z-scoreæ³•
            outliers_z = detect_outliers_zscore(combined_df, col)

            outlier_info = {
                'feature': col,
                'iqr_outliers': len(outliers_iqr),
                'zscore_outliers': len(outliers_z),
                'iqr_percentage': len(outliers_iqr) / len(combined_df) * 100,
                'zscore_percentage': len(outliers_z) / len(combined_df) * 100,
                'lower_bound': lower,
                'upper_bound': upper
            }
            outlier_summary.append(outlier_info)

    # å¤–ã‚Œå€¤ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    print("ç‰¹å¾´é‡åˆ¥å¤–ã‚Œå€¤æ¤œå‡ºçµæœ:")
    print("-" * 80)
    print(f"{'Feature':<12} {'IQR Outliers':<20} {'Z-score Outliers':<22} {'IQR Range'}")
    print("-" * 80)

    for info in outlier_summary:
        print(f"{info['feature']:<12} {info['iqr_outliers']} ({info['iqr_percentage']:.2f}%) "
              f"Z-score: {info['zscore_outliers']} ({info['zscore_percentage']:.2f}%) "
              f"ç¯„å›²: [{info['lower_bound']:.2f}, {info['upper_bound']:.2f}]")

    # åˆ†å¸ƒã®è©³ç´°åˆ†æ
    print("\n=== åˆ†å¸ƒè©³ç´°åˆ†æ ===")
    for col in key_features[:3]:  # æœ€åˆã®3ã¤ã®ç‰¹å¾´é‡ã®ã¿è©³ç´°åˆ†æ
        if col in numeric_cols:
            data = combined_df[col].dropna()
            print(f"\n{col} ã®è©³ç´°çµ±è¨ˆ:")

            # åŸºæœ¬çµ±è¨ˆ
            desc = data.describe()
            print(f"  å¹³å‡: {desc['mean']:.2f}")
            print(f"  æ¨™æº–åå·®: {desc['std']:.2f}")
            print(f"  æœ€å°å€¤: {desc['min']:.2f}")
            print(f"  ç¬¬1å››åˆ†ä½æ•°: {desc['25%']:.2f}")
            print(f"  ä¸­å¤®å€¤: {desc['50%']:.2f}")
            print(f"  ç¬¬3å››åˆ†ä½æ•°: {desc['75%']:.2f}")
            print(f"  æœ€å¤§å€¤: {desc['max']:.2f}")

            # æ­ªåº¦ã¨å°–åº¦
            skewness = data.skew()
            kurtosis = data.kurtosis()
            print(f"  æ­ªåº¦: {skewness:.4f}")
            print(f"  å°–åº¦: {kurtosis:.4f}")

            # æ­£è¦æ€§æ¤œå®š
            if len(data) > 5000:  # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒååˆ†ãªå ´åˆã®ã¿
                sample_size = min(len(data), 5000)
                _, p_value = stats.shapiro(data.sample(sample_size, random_state=42))
                print(f"  æ­£è¦æ€§æ¤œå®š på€¤: {p_value:.6f} ({'æ­£è¦åˆ†å¸ƒ' if p_value > 0.05 else 'éæ­£è¦åˆ†å¸ƒ'})")

    print("\n=== ãƒ‡ãƒ¼ã‚¿é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯ ===")
    # tsåˆ—ãŒdatetimeå‹ã§ãªã„å ´åˆã®ã¿å¤‰æ›ï¼ˆunitæŒ‡å®šã¯intå‹ã®ã¨ãã®ã¿ï¼‰
    if not pd.api.types.is_datetime64_any_dtype(combined_df['ts']):
        try:
            # intå‹ãªã‚‰unit='s'ã§å¤‰æ›ã€ãã†ã§ãªã‘ã‚Œã°unitãªã—
            if pd.api.types.is_integer_dtype(combined_df['ts'].dtype):
                combined_df['ts'] = pd.to_datetime(combined_df['ts'], unit='s')
            else:
                combined_df['ts'] = pd.to_datetime(combined_df['ts'])
        except Exception as e:
            print(f"âŒ tsåˆ—ã®datetimeå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return # å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã¯ä»¥é™ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—

    if pd.api.types.is_datetime64_any_dtype(combined_df['ts']):
        time_diffs = combined_df['ts'].sort_values().diff()
        print("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é–“éš”ã®çµ±è¨ˆ:")
        print(time_diffs.describe())
        print(f"  æœ€é »é–“éš”: {time_diffs.mode().iloc[0] if not time_diffs.mode().empty else 'N/A'}")
        print(f"  æœ€å¤§é–“éš”: {time_diffs.max()}")
        print(f"  æ¬ æé–“éš”æ•° (5åˆ†ä»¥ä¸Š): {(time_diffs > pd.Timedelta('5min')).sum()}")
    else:
        print("tsåˆ—ãŒdatetimeå‹ã§ãªã„ãŸã‚ã€é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    # ç‰¹å¾´é‡ã®ç›¸é–¢ãƒã‚§ãƒƒã‚¯
    print("\n=== ç‰¹å¾´é‡ç›¸é–¢ãƒã‚§ãƒƒã‚¯ ===")
    # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ã§ç›¸é–¢ã‚’è¨ˆç®—
    available_features = [col for col in key_features if col in combined_df.columns]
    if available_features:
        correlation_matrix = combined_df[available_features].corr()
        print("ä¸»è¦ç‰¹å¾´é‡ã®ç›¸é–¢ä¿‚æ•°:")
        print(correlation_matrix.round(3))

        # é«˜ç›¸é–¢ã®ãƒšã‚¢ã‚’ç‰¹å®š
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr = correlation_matrix.iloc[i, j]
                if isinstance(corr, (int, float)) and abs(corr) > 0.8:
                    high_corr_pairs.append((
                        correlation_matrix.columns[i],
                        correlation_matrix.columns[j],
                        corr
                    ))

        if high_corr_pairs:
            print("\né«˜ç›¸é–¢ã®ç‰¹å¾´é‡ãƒšã‚¢ (ç›¸é–¢ä¿‚æ•° > 0.8):")
            for feat1, feat2, corr in high_corr_pairs:
                print(f"{feat1} - {feat2}: ç›¸é–¢ä¿‚æ•° {corr:.3f}")
        else:
            print("\né«˜ç›¸é–¢ã®ç‰¹å¾´é‡ãƒšã‚¢ãªã—")
    else:
        print("ä¸»è¦ç‰¹å¾´é‡ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ” è©³ç´°ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹")
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§multiplierã¨config_pathã‚’å–å¾—
    import argparse
    parser = argparse.ArgumentParser(description="è©³ç´°ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument(
        '--multiplier',
        type=float,
        default=1.0,
        help='å¤–ã‚Œå€¤æ¤œå‡ºã®é–¾å€¤å€ç‡ï¼ˆä¾‹: 1.5ã«ã™ã‚‹ã¨é–¾å€¤ãŒ1.5å€ã«ãªã‚Šã¾ã™ï¼‰ã€‚ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚„ç•°å¸¸å€¤ã®è¨±å®¹åº¦ã«å¿œã˜ã¦èª¿æ•´ã—ã¦ãã ã•ã„ã€‚'
    )
    parser.add_argument(
        '--config_path',
        type=str,
        default=None,
        help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONå½¢å¼ï¼‰ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¾ã™ã€‚æœªæŒ‡å®šã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°RL_CONFIG_PATHã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚'
    )
    args = parser.parse_args()

    try:
        analyze_feature_distributions(multiplier=args.multiplier, config_path=args.config_path)
        print("\n" + "=" * 60)
        print("âœ… è©³ç´°ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†")

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()