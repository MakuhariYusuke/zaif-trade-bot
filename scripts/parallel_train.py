#!/usr/bin/env python3
# Parallel Training Script for 1M Learning
# 1Må­¦ç¿’ä¸¦åˆ—å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

import os
import sys
import json
import subprocess
import time
import psutil
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
# sys.path.append(str(Path(__file__).parent.parent.parent))

# Discordé€šçŸ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils.notify.discord import DiscordNotifier

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ParallelTrainingManager:
    """ä¸¦åˆ—å­¦ç¿’ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self):
        self.notifier = DiscordNotifier()
        self.processes = []
        self.session_id = None
        self.log_file = None
        self.log_files = []  # ç›£è¦–å¯¾è±¡ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
        self.last_log_check = time.time()
        
        # åˆ©ç”¨å¯èƒ½CPUã‚’å–å¾—
        available_cpus = list(range(psutil.cpu_count()))
        
        self.training_configs = [
            {
                'name': 'generalization',
                'config': 'config/training/prod.json',
                'priority': 'high',
                'cpu_affinity': self.calculate_cpu_affinity(0, 2, available_cpus)  # æœ€åˆã®ãƒ—ãƒ­ã‚»ã‚¹
            },
            {
                'name': 'aggressive',
                'config': 'config/training/prod_aggressive.json',
                'priority': 'low',
                'cpu_affinity': self.calculate_cpu_affinity(1, 2, available_cpus)  # 2ç•ªç›®ã®ãƒ—ãƒ­ã‚»ã‚¹
            }
        ]

    def calculate_cpu_affinity(self, process_index: int, total_processes: int, available_cpus: List[int]) -> List[int]:
        """ãƒ—ãƒ­ã‚»ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åŸºã¥ã„ã¦CPUã‚¢ãƒ•ã‚£ãƒ‹ãƒ†ã‚£ã‚’å‹•çš„ã«è¨ˆç®—"""
        cpus_per_process = len(available_cpus) // total_processes
        start = process_index * cpus_per_process
        end = start + cpus_per_process
        return available_cpus[start:end]

    def set_process_priority(self, process: subprocess.Popen, config: Dict[str, Any]):
        """ãƒ—ãƒ­ã‚»ã‚¹å„ªå…ˆåº¦è¨­å®š"""
        try:
            # Windowsã®å ´åˆã€ãƒ—ãƒ­ã‚»ã‚¹å„ªå…ˆåº¦ã‚¯ãƒ©ã‚¹ã‚’è¨­å®š
            if os.name == 'nt':  # Windows
                import win32api
                import win32process
                import win32con

                priority_class = win32con.NORMAL_PRIORITY_CLASS
                if config['priority'] == 'high':
                    priority_class = win32con.HIGH_PRIORITY_CLASS
                elif config['priority'] == 'low':
                    priority_class = win32con.IDLE_PRIORITY_CLASS

                handle = win32api.OpenProcess(win32con.PROCESS_ALL_ACCESS, False, process.pid)
                win32process.SetPriorityClass(handle, priority_class)
                win32api.CloseHandle(handle)

                logger.info(f"Set priority class for {config['name']}: {config['priority']}")
            else:
                # Unixç³»OSã®å ´åˆ
                # Use 'priority' key for both OS, map to nice value for Unix
                priority_map = {'high': -10, 'normal': 0, 'low': 10}
                nice_value = priority_map.get(config.get('priority', 'normal'), 0)
                try:
                    psutil.Process(process.pid).nice(nice_value)
                except Exception as e:
                    logger.warning(f"Failed to set nice value for {config['name']}: {e}")

            # CPUã‚¢ãƒ•ã‚£ãƒ‹ãƒ†ã‚£è¨­å®š
            if hasattr(psutil.Process, 'cpu_affinity'):
                p = psutil.Process(process.pid)
                p.cpu_affinity(config['cpu_affinity'])
                logger.info(f"Set CPU affinity for {config['name']}: {config['cpu_affinity']}")

        except Exception as e:
            logger.warning(f"Failed to set process priority for {config['name']}: {e}")

    def start_training_process(self, config: Dict[str, Any]) -> subprocess.Popen:
        """å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹"""
        cmd = [
            sys.executable,
            'scripts/train.py',
            '--config', config['config'],
            '--checkpoint-interval', '10000',
            '--checkpoint-dir', 'models/checkpoints',
            '--outlier-multiplier', '2.5'
        ]

        logger.info(f"Starting training process: {config['name']}")
        logger.info(f"Command: {' '.join(cmd)}")

        # ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            
            cwd=Path(__file__).parent  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è¨­å®š
        )

        # å„ªå…ˆåº¦è¨­å®š
        self.set_process_priority(process, config)

        return process

    def monitor_processes(self):
        """ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–"""
        while self.processes:
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ãƒã‚§ãƒƒã‚¯ï¼ˆ30åˆ†ã”ã¨ï¼‰
            current_time = time.time()
            if current_time - self.last_log_check > 1800:  # 30åˆ†
                self._check_log_updates()
                self.last_log_check = current_time

            for i, (process, config) in enumerate(self.processes):
                if process.poll() is not None:
                    # ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†
                    stdout, stderr = process.communicate()
                    exit_code = process.returncode

                    logger.info(f"Process {config['name']} finished with exit code: {exit_code}")

                    if exit_code != 0:
                        logger.error(f"Process {config['name']} stderr: {stderr}")
                        # ã‚¨ãƒ©ãƒ¼é€šçŸ¥
                        self.notifier.send_error_notification(
                            f"Training Failed: {config['name']}",
                            f"Exit code: {exit_code}\nStderr: {stderr[:500]}"
                        )
                    else:
                        logger.info(f"Process {config['name']} stdout: {stdout}")

                    # ãƒ—ãƒ­ã‚»ã‚¹ãƒªã‚¹ãƒˆã‹ã‚‰å‰Šé™¤
                    self.processes.pop(i)
                    break

            time.sleep(10)  # 10ç§’ã”ã¨ã«ç›£è¦–

    def _check_log_updates(self):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ãƒã‚§ãƒƒã‚¯"""
        try:
            # logs/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            logs_dir = Path('logs')
            if logs_dir.exists():
                for log_file in logs_dir.rglob('*.log'):
                    if time.time() - log_file.stat().st_mtime > 1800:  # 30åˆ†ä»¥ä¸Šæ›´æ–°ãªã—
                        self.notifier.send_error_notification(
                            "Log Stale Warning",
                            f"Log file {log_file.name} not updated for 30+ minutes"
                        )
        except Exception as e:
            logger.error(f"Error checking log updates: {e}")

    def send_completion_notification(self, config: Dict[str, Any]):
        """å®Œäº†é€šçŸ¥é€ä¿¡"""
        # ãƒ¢ãƒƒã‚¯çµæœãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®å­¦ç¿’çµæœã‚’å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰
        mock_results = {
            'reward_stats': {'mean_total_reward': 1000.0},
            'pnl_stats': {'mean_total_pnl': 500.0, 'max_drawdown': 0.05},
            'trading_stats': {
                'total_trades': 100,
                'winning_trades': 55,
                'profit_factor': 1.2,
                'mean_trades_per_episode': 5.0,
                'buy_ratio': 0.6,
                'sell_ratio': 0.4
            }
        }

        self.notifier.end_session(mock_results, f"{config['name']} training")

    def run_parallel_training(self):
        """ä¸¦åˆ—å­¦ç¿’å®Ÿè¡Œ"""
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDç”Ÿæˆ
        self.session_id = datetime.now().strftime('%Y%m%d-%H%M%S')
        self.log_file = Path('logs') / f'session_{self.session_id}.log'
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

        # ãƒ­ã‚°è¨­å®šã‚’æ›´æ–°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©è¿½åŠ ï¼‰
        file_handler = logging.FileHandler(self.log_file)
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        logger.addHandler(file_handler)

        logger.info("ğŸš€ Starting Parallel Training Session")
        logger.info("ğŸš€ Starting Parallel Training Session")
        logger.info(f"Session ID: {self.session_id}")
        # print("ğŸš€ Starting Parallel Training Session")  # Use logger for unified output
        # é–‹å§‹é€šçŸ¥
        session_id = self.notifier.start_session("parallel_training", "generalization + aggressive")

        try:
            # å„å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹
            for config in self.training_configs:
                process = self.start_training_process(config)
                self.processes.append((process, config))

                logger.info(f"âœ… Started {config['name']} training process (PID: {process.pid})")
                logger.info(f"âœ… Started {config['name']} training process (PID: {process.pid})")
                # print(f"âœ… Started {config['name']} training process (PID: {process.pid})")  # Use logger for unified output
            # ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°å‡ºåŠ›ï¼‰
            self.monitor_processes()

            # å…¨ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†é€šçŸ¥
            logger.info("ğŸ‰ All training processes completed!")
            logger.info("ğŸ‰ All training processes completed!")
            # print("ğŸ‰ All training processes completed!")  # Use logger for unified output
            # å®Œäº†é€šçŸ¥ï¼ˆå®Ÿéš›ã®çµæœã‚’å–å¾—ã—ã¦é€ä¿¡ï¼‰
            for config in self.training_configs:
                self.send_completion_notification(config)

        except Exception as e:
            logger.exception(f"Error in parallel training: {e}")
            self.notifier.send_error_notification("Parallel Training Error", f"Session {self.session_id}: {str(e)}")
            raise

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='Parallel Training Script')
    parser.add_argument('--timesteps', type=int, help='Override total_timesteps from config')
    args = parser.parse_args()
    
    manager = ParallelTrainingManager()
    if args.timesteps:
        # config ã® total_timesteps ã‚’ä¸Šæ›¸ã
        for config in manager.training_configs:
            config_path = Path(config['config'])
            if config_path.exists():
                with open(config_path, 'r') as f:
                    cfg = json.load(f)
                cfg['training']['total_timesteps'] = args.timesteps
                with open(config_path, 'w') as f:
                    json.dump(cfg, f, indent=2)
                logger.info(f"Updated {config['name']} total_timesteps to {args.timesteps}")
    
    manager.run_parallel_training()

if __name__ == "__main__":
    main()