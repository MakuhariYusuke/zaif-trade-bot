#!/usr/bin/env python3
# Parallel Training Script for 1M Learning
# 1MÂ≠¶Áøí‰∏¶ÂàóÂÆüË°å„Çπ„ÇØ„É™„Éó„Éà

import os
import sys
import json
import subprocess
import time
import psutil
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import logging
import threading
import queue

# „Éó„É≠„Ç∏„Çß„ÇØ„Éà„É´„Éº„Éà„Çí„Éë„Çπ„Å´ËøΩÂä†
sys.path.append(str(Path(__file__).parent.parent))

# DiscordÈÄöÁü•„É¢„Ç∏„É•„Éº„É´„ÅÆ„Ç§„É≥„Éù„Éº„Éà
from utils.notify.discord import DiscordNotifier

# „É≠„Ç∞Ë®≠ÂÆö
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def determine_process_count():
    """„Éó„É≠„Çª„ÇπÊï∞„ÇíËá™ÂãïÊ±∫ÂÆöÔºàÁâ©ÁêÜ„Ç≥„Ç¢Êï∞„Å®Á©∫„Åç„É°„É¢„É™„ÇíËÄÉÊÖÆÔºâ"""
    physical = psutil.cpu_count(logical=False) or 1
    avail_gb = psutil.virtual_memory().available / (1024**3)
    base = max(1, physical // 2)
    mem_cap = max(1, int(avail_gb // 2))
    return min(base, mem_cap, 4)  # ‰∏äÈôê4

def _pump_stream(name, stream, cfg, logger):
    """„Çπ„Éà„É™„Éº„É†„ÇíÈùûÂêåÊúü„ÅßË™≠„ÅøÂèñ„ÇãÔºàWindowsÂØæÂøúÔºâ"""
    for line in iter(stream.readline, ''):
        line = line.rstrip('\n')
        if stream is cfg["proc"].stdout:
            logger.info(f"[{cfg['name']}] {line}")
        else:
            logger.error(f"[{cfg['name']}] {line}")

class ParallelTrainingManager:
    """‰∏¶ÂàóÂ≠¶Áøí„Éû„Éç„Éº„Ç∏„É£„Éº"""

    def __init__(self):
        # Áí∞Â¢ÉË®≠ÂÆö„ÅÆË™≠„ÅøËæº„Åø
        self.env_config = self._load_env_config()
        
        self.notifier = DiscordNotifier()
        self.processes = []
        self.session_id = None
        self.log_file = None
        self.log_files = []  # Áõ£Ë¶ñÂØæË±°„É≠„Ç∞„Éï„Ç°„Ç§„É´
        self.last_log_check = time.time()
        
        # „Éó„É≠„Çª„ÇπÊï∞„ÇíËá™ÂãïÊ±∫ÂÆö
        self.num_processes = determine_process_count()
        os.environ['PARALLEL_PROCESSES'] = str(self.num_processes)
        
        # Âà©Áî®ÂèØËÉΩCPU„ÇíÂèñÂæó
        available_cpus = list(range(psutil.cpu_count(logical=False) or 1))
        
        self.training_configs = [
            {
                'name': 'generalization',
                'config': 'config/training/prod.json',
                'priority': 'high',
                'cpu_affinity': self.calculate_cpu_affinity(0, self.num_processes, available_cpus),
                'process_index': 0
            },
            {
                'name': 'aggressive',
                'config': 'config/training/prod_aggressive.json',
                'priority': 'low',
                'cpu_affinity': self.calculate_cpu_affinity(1, self.num_processes, available_cpus),
                'process_index': 1
            }
        ]

    def _load_env_config(self) -> dict:
        """Áí∞Â¢ÉË®≠ÂÆö„ÇíË™≠„ÅøËæº„Åø"""
        env = os.environ.get('ENV', 'dev')
        env_config_path = Path(f'config/environment/{env}.json')
        if env_config_path.exists():
            with open(env_config_path, 'r') as f:
                return json.load(f)
        return {}

    def calculate_cpu_affinity(self, process_index: int, total_processes: int, available_cpus: List[int]) -> List[int]:
        """„Éó„É≠„Çª„Çπ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Å´Âü∫„Å•„ÅÑ„Å¶CPU„Ç¢„Éï„Ç£„Éã„ÉÜ„Ç£„ÇíÂãïÁöÑ„Å´Ë®àÁÆó"""
        cpus_per_process = len(available_cpus) // total_processes
        start = process_index * cpus_per_process
        end = start + cpus_per_process
        return available_cpus[start:end]

    def set_process_priority(self, process: subprocess.Popen, config: Dict[str, Any]):
        """„Éó„É≠„Çª„ÇπÂÑ™ÂÖàÂ∫¶Ë®≠ÂÆö"""
        try:
            # Windows„ÅÆÂ†¥Âêà„ÄÅ„Éó„É≠„Çª„ÇπÂÑ™ÂÖàÂ∫¶„ÇØ„É©„Çπ„ÇíË®≠ÂÆö
            if os.name == 'nt':  # Windows
                import win32api
                import win32process
                import win32con

                priority_class = win32con.NORMAL_PRIORITY_CLASS
                if config['priority'] == 'high':
                    priority_class = win32con.HIGH_PRIORITY_CLASS
                elif config['priority'] == 'low':
                    priority_class = win32con.IDLE_PRIORITY_CLASS

                handle = win32api.OpenProcess(win32con.PROCESS_ALL_ACCESS, False, process.pid)
                win32process.SetPriorityClass(handle, priority_class)
                win32api.CloseHandle(handle)

                logger.info(f"Set priority class for {config['name']}: {config['priority']}")
            else:
                # UnixÁ≥ªOS„ÅÆÂ†¥Âêà
                # Use 'priority' key for both OS, map to nice value for Unix
                priority_map = {'high': -10, 'normal': 0, 'low': 10}
                nice_value = priority_map.get(config.get('priority', 'normal'), 0)
                try:
                    psutil.Process(process.pid).nice(nice_value)
                except Exception as e:
                    logger.warning(f"Failed to set nice value for {config['name']}: {e}")

            # CPU„Ç¢„Éï„Ç£„Éã„ÉÜ„Ç£Ë®≠ÂÆö
            if hasattr(psutil.Process, 'cpu_affinity'):
                p = psutil.Process(process.pid)
                p.cpu_affinity(config['cpu_affinity'])
                logger.info(f"Set CPU affinity for {config['name']}: {config['cpu_affinity']}")

        except Exception as e:
            logger.warning(f"Failed to set process priority for {config['name']}: {e}")

    def start_training_process(self, config: Dict[str, Any]) -> subprocess.Popen:
        """Â≠¶Áøí„Éó„É≠„Çª„Çπ„ÇíÈñãÂßã"""
        cmd = [
            sys.executable,
            'train.py',
            '--config', config['config'],
            '--checkpoint-interval', '10000',
            '--checkpoint-dir', 'models/checkpoints',
            '--outlier-multiplier', '2.5'
        ]

        logger.info(f"Starting training process: {config['name']}")
        logger.info(f"Command: {' '.join(cmd)}")

        # Áí∞Â¢ÉÂ§âÊï∞„ÇíÂ≠ê„Éó„É≠„Çª„Çπ„Å´Ê∏°„Åô
        env = os.environ.copy()
        env['PARALLEL_PROCESSES'] = str(self.num_processes)
        env['PROCESS_ID'] = str(config['process_index'])
        env['CPU_AFFINITY'] = ','.join(map(str, config['cpu_affinity']))

        # „Éó„É≠„Çª„ÇπÈñãÂßã
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            env=env,
            cwd=Path(__file__).parent  # „Éó„É≠„Ç∏„Çß„ÇØ„Éà„É´„Éº„Éà„Çí„Ç´„É¨„É≥„Éà„Éá„Ç£„É¨„ÇØ„Éà„É™„Å´Ë®≠ÂÆö
        )

        # ÂÑ™ÂÖàÂ∫¶Ë®≠ÂÆö
        self.set_process_priority(process, config)

        return process

    def monitor_processes(self):
        """„Éó„É≠„Çª„ÇπÁõ£Ë¶ñÔºà„Çπ„É¨„ÉÉ„ÉâÂåñ„Åï„Çå„Åü„Çπ„Éà„É™„Éº„É†Ë™≠Âèñ„Çí‰ΩøÁî®Ôºâ"""
        while self.processes:
            # „É≠„Ç∞„Éï„Ç°„Ç§„É´Êõ¥Êñ∞„ÉÅ„Çß„ÉÉ„ÇØÔºà30ÂàÜ„Åî„Å®Ôºâ
            current_time = time.time()
            if current_time - self.last_log_check > 1800:  # 30ÂàÜ
                self._check_log_updates()
                self.last_log_check = current_time

            for i, (process, config) in enumerate(self.processes):
                if process.poll() is not None:
                    # „Éó„É≠„Çª„ÇπÁµÇ‰∫Ü
                    exit_code = process.returncode

                    logger.info(f"Process {config['name']} finished with exit code: {exit_code}")

                    if exit_code != 0:
                        # „Ç®„É©„ÉºÈÄöÁü•Ôºà„Çπ„Éà„É™„Éº„É†„ÅØ„Çπ„É¨„ÉÉ„Éâ„ÅßÊó¢„Å´Ë™≠„Åæ„Çå„Å¶„ÅÑ„ÇãÔºâ
                        self.notifier.send_error_notification(
                            f"Training Failed: {config['name']}",
                            f"Exit code: {exit_code}"
                        )

                    # „Éó„É≠„Çª„Çπ„É™„Çπ„Éà„Åã„ÇâÂâäÈô§
                    self.processes.pop(i)
                    break

            time.sleep(10)  # 10Áßí„Åî„Å®„Å´Áõ£Ë¶ñ

    def _check_log_updates(self):
        """„É≠„Ç∞„Éï„Ç°„Ç§„É´Êõ¥Êñ∞„ÉÅ„Çß„ÉÉ„ÇØ"""
        try:
            # logs/ „Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖ„ÅÆ„É≠„Ç∞„Éï„Ç°„Ç§„É´„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            logs_dir = Path('logs')
            if logs_dir.exists():
                for log_file in logs_dir.rglob('*.log'):
                    if time.time() - log_file.stat().st_mtime > 1800:  # 30ÂàÜ‰ª•‰∏äÊõ¥Êñ∞„Å™„Åó
                        self.notifier.send_error_notification(
                            "Log Stale Warning",
                            f"Log file {log_file.name} not updated for 30+ minutes"
                        )
        except Exception as e:
            logger.error(f"Error checking log updates: {e}")

    def send_completion_notification(self, config: Dict[str, Any]):
        """ÂÆå‰∫ÜÈÄöÁü•ÈÄÅ‰ø°"""
        # „É¢„ÉÉ„ÇØÁµêÊûú„Éá„Éº„ÇøÔºàÂÆüÈöõ„ÅÆÂ≠¶ÁøíÁµêÊûú„ÇíÂèñÂæó„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„ÇãÔºâ
        mock_results = {
            'reward_stats': {'mean_total_reward': 1000.0},
            'pnl_stats': {'mean_total_pnl': 500.0, 'max_drawdown': 0.05},
            'trading_stats': {
                'total_trades': 100,
                'winning_trades': 55,
                'profit_factor': 1.2,
                'mean_trades_per_episode': 5.0,
                'buy_ratio': 0.6,
                'sell_ratio': 0.4
            }
        }

        self.notifier.end_session(mock_results, f"{config['name']} training")

    def run_parallel_training(self):
        """‰∏¶ÂàóÂ≠¶ÁøíÂÆüË°å"""
        # „Çª„ÉÉ„Ç∑„Éß„É≥IDÁîüÊàê
        self.session_id = datetime.now().strftime('%Y%m%d-%H%M%S')
        self.log_file = Path('logs') / f'session_{self.session_id}.log'
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

        # „É≠„Ç∞Ë®≠ÂÆö„ÇíÊõ¥Êñ∞Ôºà„Éï„Ç°„Ç§„É´„Éè„É≥„Éâ„É©ËøΩÂä†Ôºâ
        file_handler = logging.FileHandler(self.log_file)
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        logger.addHandler(file_handler)

        logger.info("üöÄ Starting Parallel Training Session")
        logger.info("üöÄ Starting Parallel Training Session")
        logger.info(f"Session ID: {self.session_id}")
        # print("üöÄ Starting Parallel Training Session")  # Use logger for unified output
        # ÈñãÂßãÈÄöÁü•
        session_id = self.notifier.start_session("parallel_training", "generalization + aggressive")

        try:
            # ÂêÑÂ≠¶Áøí„Éó„É≠„Çª„Çπ„ÇíÈñãÂßã
            for config in self.training_configs:
                process = self.start_training_process(config)
                self.processes.append((process, config))

                # „Çπ„Éà„É™„Éº„É†Ë™≠Âèñ„Çπ„É¨„ÉÉ„Éâ„ÇíËµ∑Âãï
                t_out = threading.Thread(target=_pump_stream, args=("stdout", process.stdout, {"proc": process, "name": config["name"]}, logger), daemon=True)
                t_err = threading.Thread(target=_pump_stream, args=("stderr", process.stderr, {"proc": process, "name": config["name"]}, logger), daemon=True)
                t_out.start()
                t_err.start()

                logger.info(f"‚úÖ Started {config['name']} training process (PID: {process.pid})")
                logger.info(f"proc_name={config['name']}, pid={process.pid}, affinity={config['cpu_affinity']}")
                logger.info(f"‚úÖ Started {config['name']} training process (PID: {process.pid})")
                # print(f"‚úÖ Started {config['name']} training process (PID: {process.pid})")  # Use logger for unified output
            # „Éó„É≠„Çª„ÇπÁõ£Ë¶ñÔºà„É™„Ç¢„É´„Çø„Ç§„É†„É≠„Ç∞Âá∫ÂäõÔºâ
            self.monitor_processes()

            # ÂÖ®„Éó„É≠„Çª„ÇπÂÆå‰∫ÜÈÄöÁü•
            logger.info("üéâ All training processes completed!")
            logger.info("üéâ All training processes completed!")
            # print("üéâ All training processes completed!")  # Use logger for unified output
            # ÂÆå‰∫ÜÈÄöÁü•ÔºàÂÆüÈöõ„ÅÆÁµêÊûú„ÇíÂèñÂæó„Åó„Å¶ÈÄÅ‰ø°Ôºâ
            for config in self.training_configs:
                self.send_completion_notification(config)

        except Exception as e:
            logger.exception(f"Error in parallel training: {e}")
            self.notifier.send_error_notification("Parallel Training Error", f"Session {self.session_id}: {str(e)}")
            raise

def main():
    """„É°„Ç§„É≥Èñ¢Êï∞"""
    parser = argparse.ArgumentParser(description='Parallel Training Script')
    parser.add_argument('--timesteps', type=int, help='Override total_timesteps from config')
    args = parser.parse_args()
    
    # Safety Clamp: PRODUCTION=1„Åß„Å™„ÅÑÈôê„Çä„ÄÅtotal_timesteps„Çí1000„Å´Âº∑Âà∂
    is_production = os.environ.get('PRODUCTION') == '1'
    if not is_production and args.timesteps and args.timesteps > 1000:
        args.timesteps = 1000
        logger.warning("[SAFETY] Non-production run: total_timesteps clamped to 1000")
    
    manager = ParallelTrainingManager()
    if args.timesteps:
        # config „ÅÆ total_timesteps „Çí‰∏äÊõ∏„Åç
        for config in manager.training_configs:
            config_path = Path(config['config'])
            if config_path.exists():
                with open(config_path, 'r') as f:
                    cfg = json.load(f)
                cfg['training']['total_timesteps'] = args.timesteps
                with open(config_path, 'w') as f:
                    json.dump(cfg, f, indent=2)
                logger.info(f"Updated {config['name']} total_timesteps to {args.timesteps}")
    
    manager.run_parallel_training()

if __name__ == "__main__":
    main()