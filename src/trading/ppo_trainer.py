# PPO Training Script for Heavy Trading Environment
# é‡ç‰¹å¾´é‡å–å¼•ç’°å¢ƒã§ã®PPOå­¦ç¿’

import os
import sys
import numpy as np
import pandas as pd
from pathlib import Path
import torch
import psutil
from typing import Dict, Any, Optional
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.logger import configure
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
import logging
import gzip
from logging.handlers import BufferingHandler
import concurrent.futures
import threading

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# sys.path.append(str(Path(__file__).parent.parent.parent.parent))  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
from .environment import HeavyTradingEnv
from ..utils.perf.cpu_tune import apply_cpu_tuning

# éåŒæœŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ç”¨
_save_executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
_save_lock = threading.Lock()
_keep_last = 5  # ä¿æŒä¸–ä»£æ•°

def save_checkpoint_async(model, path_base: str, notifier=None):
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’éåŒæœŸã§ä¿å­˜ï¼ˆåŸå­çš„ãƒ»ä¸–ä»£ç®¡ç†ä»˜ãï¼‰"""
    def _job():
        try:
            # tmpã«ä¿å­˜
            tmp_path = f"{path_base}.tmp"
            final_path = f"{path_base}.zip"
            
            with _save_lock:
                model.save(tmp_path)
                # åŸå­çš„rename
                os.replace(tmp_path, final_path)
            
            logging.info(f"[CKPT] saved: {final_path}")
            
            # ä¸–ä»£ç®¡ç†: åŒã˜prefixã®å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            dir_path = Path(final_path).parent
            prefix = Path(final_path).stem.split('_')[0] if '_' in Path(final_path).stem else Path(final_path).stem
            
            checkpoints = sorted(dir_path.glob(f"{prefix}_*.zip"), key=lambda x: x.stat().st_mtime, reverse=True)
            for old_ckpt in checkpoints[_keep_last:]:
                old_ckpt.unlink()
                logging.info(f"[CKPT] removed old: {old_ckpt}")
                
        except Exception as e:
            logging.exception(f"[CKPT] save failed: {e}")
            if notifier:
                notifier.send_error_notification("Checkpoint Save Error", f"Failed to save {path_base}: {str(e)}")
    
    _save_executor.submit(_job)

class TensorBoardCallback(BaseCallback):
    """TensorBoardç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""

    def __init__(self, eval_freq: int = 1000, verbose: int = 0):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿

        Args:
            eval_freq (int): è©•ä¾¡é »åº¦ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼‰
            verbose (int): è©³ç´°ãƒ­ã‚°ã®ãƒ¬ãƒ™ãƒ«
        """
        super().__init__(verbose)
        self.eval_freq = eval_freq

    def _on_step(self) -> bool:
        """
        å„ã‚¹ãƒ†ãƒƒãƒ—ã§å‘¼ã³å‡ºã•ã‚Œã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã€‚ç’°å¢ƒã®çµ±è¨ˆæƒ…å ±ã‚’TensorBoardã«è¨˜éŒ²ã—ã¾ã™ã€‚

        Returns:
            bool: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç¶™ç¶šã™ã‚‹å ´åˆã¯True
        """
        if self.n_calls % self.eval_freq == 0 and self.model.env:
            # ç’°å¢ƒã®çµ±è¨ˆæƒ…å ±ã‚’TensorBoardã«è¨˜éŒ²
            # VecEnvã‹ã‚‰ get_statistics ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç›´æ¥å–å¾—
            stats_list = self.model.env.get_attr('get_statistics')
            if stats_list and callable(stats_list[0]):
                stats = stats_list[0]()
                if isinstance(stats, dict):
                    for key, value in stats.items():
                        self.logger.record(f'env/{key}', value)
        return True


class CheckpointCallback(BaseCallback):
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""

    def __init__(self, save_freq: int, save_path: str, name_prefix: str = "checkpoint", verbose: int = 0, notifier=None, session_id=None):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿

        Args:
            save_freq (int): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹é »åº¦ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼‰
            save_path (str): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ãƒ‘ã‚¹
            name_prefix (str): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            verbose (int): è©³ç´°ãƒ­ã‚°ã®ãƒ¬ãƒ™ãƒ«
            notifier: é€šçŸ¥ç”¨ã®ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãªNotifierã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            session_id: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ID
        """
        super().__init__(verbose)
        self.save_freq = save_freq
        self.save_path = Path(save_path)
        self.save_path.mkdir(parents=True, exist_ok=True)
        self.name_prefix = name_prefix
        self.notifier = notifier
        self.session_id = session_id

    def _on_step(self) -> bool:
        """
        å„ã‚¹ãƒ†ãƒƒãƒ—ã§å‘¼ã³å‡ºã•ã‚Œã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã€‚å®šæœŸçš„ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚

        Returns:
            bool: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç¶™ç¶šã™ã‚‹å ´åˆã¯True
        """
        try:
            if self.n_calls % self.save_freq == 0:
                checkpoint_path = self.save_path / f"{self.name_prefix}_{self.n_calls}"
                # éåŒæœŸä¿å­˜
                save_checkpoint_async(self.model, str(checkpoint_path), self.notifier)
                
                total_timesteps = getattr(self.model, "_total_timesteps", 1000000)
                progress_percent = (self.n_calls / total_timesteps) * 100
                progress_msg = f"Step {self.n_calls:,} / {self.n_calls:,} ({progress_percent:.1f}%)"

                # INFOãƒ­ã‚°
                logging.info(progress_msg)

                # Discordé€šçŸ¥ï¼ˆ10%ã”ã¨ï¼‰
                if int(progress_percent) % 10 == 0 and self.notifier:
                    self.notifier.send_custom_notification(
                        f"ğŸ“Š Training Progress ({self.session_id})",
                        progress_msg,
                        color=0x00ff00
                    )

                if self.verbose > 0:
                    print(progress_msg)

        except Exception as e:
            logging.error(f"Error in checkpoint callback: {e}")
            if self.notifier:
                self.notifier.send_error_notification("Checkpoint Error", str(e))

        return True


class SafetyCallback(BaseCallback):
    """å®‰å…¨ç­–ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ - ãƒˆãƒ¬ãƒ¼ãƒ‰æ•°ãŒ0ã®ã¾ã¾å­¦ç¿’ã‚’åœæ­¢"""

    def __init__(self, max_zero_trades=10000, verbose=0):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿

        Args:
            max_zero_trades (int): ãƒˆãƒ¬ãƒ¼ãƒ‰æ•°ãŒ0ã®ã¾ã¾è¨±å®¹ã™ã‚‹æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°
            verbose (int): è©³ç´°ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
        """
        super().__init__(verbose)
        self.max_zero_trades = max_zero_trades
        self.zero_trade_count = 0

    def _on_step(self) -> bool:
        """
        å„ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒˆãƒ¬ãƒ¼ãƒ‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€0ã®ã¾ã¾ç¶šãã¨å­¦ç¿’ã‚’åœæ­¢

        Returns:
            bool: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç¶™ç¶šã™ã‚‹å ´åˆã¯True
        """
        try:
            # ç’°å¢ƒã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
            if self.model.env:
                stats_list = self.model.env.get_attr('get_statistics')
                if stats_list and callable(stats_list[0]):
                    stats = stats_list[0]()
                    if isinstance(stats, dict):
                        total_trades = stats.get('total_trades', 0)

                        if total_trades == 0:
                            self.zero_trade_count += 1
                            if self.zero_trade_count >= self.max_zero_trades:
                                logging.warning(f"No trades for {self.max_zero_trades} steps, stopping training")
                                return False  # å­¦ç¿’åœæ­¢
                        else:
                            self.zero_trade_count = 0

        except Exception as e:
            logging.error(f"Error in safety callback: {e}")

        return True


class PPOTrainer:
    """PPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self, data_path: str, config: Optional[dict] = None, checkpoint_interval: int = 10000, checkpoint_dir: str = 'models/checkpoints'):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿

        Args:
            data_path (str): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
            config (Optional[dict]): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
            checkpoint_interval (int): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã®é–“éš”ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼‰
            checkpoint_dir (str): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        # CPUæœ€é©åŒ–ã‚’æœ€åˆã«é©ç”¨
        apply_cpu_tuning()
        
        self.data_path = Path(data_path)
        
        # CPUæœ€é©åŒ–è¨­å®š
        self._setup_cpu_optimization()
        
        self.config = config or self._get_default_config()
        self.checkpoint_interval = checkpoint_interval
        self.checkpoint_dir = Path(checkpoint_dir)

        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        self.log_dir = Path(self.config['log_dir'])
        self.log_dir.mkdir(exist_ok=True)
        # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.model_dir = Path(self.config['model_dir'])
        self.model_dir.mkdir(exist_ok=True)

        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        self.df = self._load_data()

        # ç’°å¢ƒã®ä½œæˆ
        self.env = self._create_env()

    def _setup_cpu_optimization(self) -> None:
        """CPUæœ€é©åŒ–è¨­å®š"""
        from ..utils.perf.cpu_tune import auto_config_threads
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šå–å¾—
        num_processes = int(os.environ.get("PARALLEL_PROCESSES", "1"))
        pin_cores_str = os.environ.get("CPU_AFFINITY")
        pin_to_cores = [int(x) for x in pin_cores_str.split(",")] if pin_cores_str else None
        
        # è‡ªå‹•è¨­å®šæ±ºå®š
        cpu_config = auto_config_threads(num_processes, pin_to_cores)
        
        # ç’°å¢ƒå¤‰æ•°è¨­å®š
        for key, value in cpu_config.items():
            if key.startswith(('OMP_', 'MKL_', 'OPENBLAS_', 'NUMEXPR_')):
                os.environ[key] = str(value)
        
        # PyTorchè¨­å®š
        torch.set_num_threads(cpu_config['torch_threads'])
        torch.backends.mkldnn.enabled = True
        
        # ãƒ­ã‚°å‡ºåŠ›
        logging.info(f"CPU: phys={cpu_config['physical_cores']}, log={cpu_config['logical_cores']}, "
                     f"procs={cpu_config['num_processes']}, pin={cpu_config['pin_to_cores']}, "
                     f"torch={cpu_config['torch_threads']}, OMP={cpu_config['OMP_NUM_THREADS']}, "
                     f"MKL={cpu_config['MKL_NUM_THREADS']}, OPENBLAS={cpu_config['OPENBLAS_NUM_THREADS']}")

    def _get_default_config(self) -> dict:
        """
        ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®PPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã‚’è¿”ã—ã¾ã™ã€‚

        Returns:
            dict: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®è¾æ›¸
        """
        return {
            'total_timesteps': 200000,  # æœ¬ç•ªç”¨ã¨åŒã˜å€¤ã«çµ±ä¸€
            'eval_freq': 5000,
            'n_eval_episodes': 5,
            'batch_size': 64,
            'n_steps': 2048,
            'gamma': 0.99,
            'learning_rate': 3e-4,
            'ent_coef': 0.01,
            'clip_range': 0.2,
            'n_epochs': 10,
            'gae_lambda': 0.95,
            'max_grad_norm': 0.5,
            'vf_coef': 0.5,
            'log_dir': './logs/',
            'model_dir': './models/',
            'tensorboard_log': './tensorboard/',
            'verbose': 1,
            'seed': 42,
        }

    def _load_data(self) -> pd.DataFrame:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
        ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ‘ã‚¹ã«ã‚‚å¯¾å¿œã—ã€è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã§ãã¾ã™ã€‚

        Returns:
            pd.DataFrame: èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 

        Raises:
            FileNotFoundError: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
            ValueError: ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã¾ãŸã¯æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆ
        """
        data_path = Path(self.data_path)

        # ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹å ´åˆ
        if '*' in str(data_path):
            # globãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            import glob
            file_paths = glob.glob(str(data_path))

            if not file_paths:
                raise FileNotFoundError(f"No files found matching pattern: {data_path}")

            print(f"Found {len(file_paths)} files matching pattern: {data_path}")

            # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§çµåˆ
            dfs = []
            for file_path in file_paths:
                file_path = Path(file_path)
                if file_path.suffix == '.parquet':
                    df = pd.read_parquet(file_path)
                elif file_path.suffix == '.csv':
                    df = pd.read_csv(file_path)
                else:
                    print(f"Skipping unsupported file: {file_path}")
                    continue

                dfs.append(df)
                print(f"Loaded {file_path.name}: {len(df)} rows")

            if not dfs:
                raise ValueError("No valid data files found")

            # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            df = pd.concat(dfs, ignore_index=True)

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ
            if 'ts' in df.columns:
                df = df.sort_values('ts').reset_index(drop=True)

        else:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
            if not data_path.exists():
                raise FileNotFoundError(f"Data file not found: {data_path}")

            if data_path.suffix == '.parquet':
                df = pd.read_parquet(data_path)
            elif data_path.suffix == '.csv':
                df = pd.read_csv(data_path)
            else:
                raise ValueError(f"Unsupported file format: {data_path.suffix}")

        print(f"Total loaded data: {len(df)} rows, {len(df.columns)} columns")
        print(f"Columns: {list(df.columns)}")

        return df

    def _create_env(self):
        """
        ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®HeavyTradingEnvç’°å¢ƒã‚’ä½œæˆã—ã€Monitorã§ãƒ©ãƒƒãƒ—ã—ã¾ã™ã€‚
        è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¼•æ‰‹æ•°æ–™ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

        Returns:
            Monitor: ãƒ¢ãƒ‹ã‚¿ãƒ¼ã§ãƒ©ãƒƒãƒ—ã•ã‚ŒãŸç’°å¢ƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        # rl_config.jsonã‹ã‚‰æ‰‹æ•°æ–™è¨­å®šã‚’èª­ã¿è¾¼ã¿
        # rl_config.jsonã‹ã‚‰æ‰‹æ•°æ–™è¨­å®šã‚’èª­ã¿è¾¼ã¿
        config_path = os.environ.get("RL_CONFIG_PATH")
        if config_path is None:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
            config_path = str(Path(__file__).parent.parent.parent.parent / "rl_config.json")
        config_path = Path(config_path)
        transaction_cost = 0.001  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        if config_path.exists():
            with open(config_path, 'r') as f:
                rl_config = json.load(f)
            fee_config = rl_config.get('fee_model', {})
            transaction_cost = fee_config.get('default_fee_rate', 0.001)
        env_config = {
            'reward_scaling': 1.0,
            'transaction_cost': transaction_cost,
            'max_position_size': 1.0,
            'risk_free_rate': 0.0,
        }

        env = HeavyTradingEnv(self.df, env_config)
        env = Monitor(env, str(self.log_dir / 'monitor.csv'))

        return env

    def train(self, notifier=None, session_id=None) -> PPO:
        """
        è¨­å®šã«åŸºã¥ã„ã¦PPOãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚

        Args:
            notifier: é€šçŸ¥ç”¨ã®ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãªNotifierã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            session_id: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ID

        Returns:
            PPO: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®PPOãƒ¢ãƒ‡ãƒ«

        Raises:
            Exception: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        """
        # I/Oæœ€é©åŒ–: ãƒ­ã‚°ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã‚’è¨­å®š
        buffer_handler = BufferingHandler(1000)  # 1000ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã”ã¨ã«ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
        buffer_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        buffer_handler.setFormatter(formatter)
        logging.getLogger().addHandler(buffer_handler)
        
        logging.info("Starting PPO training...")
        print("Starting PPO training...")

        # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
        model = PPO(
            'MlpPolicy',
            self.env,
            learning_rate=self.config['learning_rate'],
            n_steps=self.config['n_steps'],
            batch_size=self.config['batch_size'],
            n_epochs=self.config['n_epochs'],
            gamma=self.config['gamma'],
            gae_lambda=self.config['gae_lambda'],
            clip_range=self.config['clip_range'],
            ent_coef=self.config['ent_coef'],
            vf_coef=self.config['vf_coef'],
            max_grad_norm=self.config['max_grad_norm'],
            tensorboard_log=self.config['tensorboard_log'],
            verbose=self.config['verbose'],
            seed=self.config['seed'],
        )

        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®è¨­å®š
        eval_callback = EvalCallback(
            self.env,
            best_model_save_path=str(self.model_dir / 'best_model'),
            log_path=str(self.log_dir / 'eval'),
            eval_freq=self.config['eval_freq'],
            n_eval_episodes=self.config['n_eval_episodes'],
            deterministic=True,
            render=False,
        )

        tensorboard_callback = TensorBoardCallback(eval_freq=self.config['eval_freq'])

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®è¨­å®š
        checkpoint_callback = CheckpointCallback(
            save_freq=self.checkpoint_interval,
            save_path=str(self.checkpoint_dir),
            name_prefix=f"ppo_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            verbose=1,
            notifier=notifier,
            session_id=session_id
        )

        # å®‰å…¨ç­–ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®è¨­å®š
        safety_callback = SafetyCallback(max_zero_trades=1000, verbose=1)

        try:
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ
            logging.info(f"Training started with total_timesteps: {self.config['total_timesteps']}")
            model.learn(
                total_timesteps=self.config['total_timesteps'],
                callback=[eval_callback, tensorboard_callback, checkpoint_callback, safety_callback],
                progress_bar=True,
            )

            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            model.save(str(self.model_dir / 'final_model'))
            logging.info(f"Model saved to {self.model_dir / 'final_model'}")
            print(f"Model saved to {self.model_dir / 'final_model'}")

            # I/Oæœ€é©åŒ–: ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
            buffer_handler.flush()
            
            return model

        except Exception as e:
            logging.exception(f"Training failed: {e}")
            # I/Oæœ€é©åŒ–: ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
            buffer_handler.flush()
            if notifier:
                notifier.send_error_notification("Training Failed", f"Session {session_id}: {str(e)}")
            raise

    def evaluate(self, model_path: Optional[str] = None, n_episodes: int = 10) -> dict:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€çµ±è¨ˆæƒ…å ±ã‚’è¿”ã—ã¾ã™ã€‚

        Args:
            model_path (Optional[str]): è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã€‚Noneã®å ´åˆã¯æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã€‚
            n_episodes (int): è©•ä¾¡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°

        Returns:
            dict: è©•ä¾¡çµæœã®çµ±è¨ˆæƒ…å ±ï¼ˆå¹³å‡å ±é…¬ãªã©ï¼‰
        """
        if model_path is None:
            model_path = str(self.model_dir / 'best_model')

        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        model = PPO.load(model_path)

        # è©•ä¾¡ç’°å¢ƒã®ä½œæˆ
        eval_env = DummyVecEnv([lambda: self._create_env()])

        # è©•ä¾¡ã®å®Ÿè¡Œ
        episode_rewards = []
        episode_lengths = []

        for episode in range(n_episodes):
            obs = eval_env.reset()
            done = False
            episode_reward = 0
            episode_length = 0

            while not done:
                # obsãŒã‚¿ãƒ—ãƒ«ã®å ´åˆã€æœ€åˆã®è¦ç´ ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’ä½¿ç”¨
                predict_obs = obs[0] if isinstance(obs, tuple) else obs
                action, _ = model.predict(predict_obs, deterministic=True)
                obs, reward, done_vec, info = eval_env.step(action)
                done = done_vec[0]
                episode_reward += reward[0]
                episode_length += 1

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            print(f"Episode {episode + 1}: Reward = {episode_reward:.2f}, Length = {episode_length}")

        # çµ±è¨ˆã®è¨ˆç®—
        stats = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'min_reward': np.min(episode_rewards),
            'max_reward': np.max(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'total_episodes': n_episodes,
        }

        # çµæœã®ä¿å­˜
        results_path = self.log_dir / 'evaluation_results.json'
        with open(results_path, 'w') as f:
            json.dump(stats, f, indent=2)

        print(f"Evaluation results saved to {results_path}")
        return stats

    def visualize_training(self) -> None:
        """
        monitor.csvãƒ­ã‚°ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã‚’å¯è¦–åŒ–ã—ã€ç”»åƒã‚’ä¿å­˜ã—ã¾ã™ã€‚
        """
        # ãƒ¢ãƒ‹ã‚¿ãƒ¼ãƒ­ã‚°ã®èª­ã¿è¾¼ã¿
        monitor_file = self.log_dir / 'monitor.csv'
        if monitor_file.exists():
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œæ•°ã‚’è‡ªå‹•åˆ¤å®š
            with open(monitor_file, 'r', encoding='utf-8') as f:
                header_lines = 0
                for line in f:
                    if line.startswith('#'):
                        header_lines += 1
                    else:
                        break
            monitor_df = pd.read_csv(monitor_file, skiprows=header_lines)

            # ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))

            # ãƒªãƒ¯ãƒ¼ãƒ‰ã®æ¨ç§»
            axes[0, 0].plot(monitor_df['r'], alpha=0.7)
            axes[0, 0].set_title('Episode Rewards')
            axes[0, 0].set_xlabel('Episode')
            axes[0, 0].set_ylabel('Reward')
            axes[0, 0].grid(True)

            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã®æ¨ç§»
            axes[0, 1].plot(monitor_df['l'], alpha=0.7)
            axes[0, 1].set_title('Episode Lengths')
            axes[0, 1].set_xlabel('Episode')
            axes[0, 1].set_ylabel('Length')
            axes[0, 1].grid(True)

            # ãƒªãƒ¯ãƒ¼ãƒ‰ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
            axes[1, 0].hist(monitor_df['r'], bins=50, alpha=0.7)
            axes[1, 0].set_title('Reward Distribution')
            axes[1, 0].set_xlabel('Reward')
            axes[1, 0].set_ylabel('Frequency')
            axes[1, 0].grid(True)

            # ç´¯ç©ãƒªãƒ¯ãƒ¼ãƒ‰
            axes[1, 1].plot(np.cumsum(monitor_df['r']), alpha=0.7)
            axes[1, 1].set_title('Cumulative Rewards')
            axes[1, 1].set_xlabel('Episode')
            axes[1, 1].set_ylabel('Cumulative Reward')
            axes[1, 1].grid(True)

            plt.tight_layout()
            plt.savefig(self.log_dir / 'training_visualization.png', dpi=300, bbox_inches='tight')
            plt.show()

            print(f"Training visualization saved to {self.log_dir / 'training_visualization.png'}")


def optimize_hyperparameters(data_path: str, n_trials: int = 50) -> dict:
    """
    Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
    :param data_path: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
    :param n_trials: è©¦è¡Œå›æ•°  
    """

    def objective(trial):
        """
        Optunaã®ç›®çš„é–¢æ•°ã€‚æŒ‡å®šã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

        Args:
            trial: Optunaã®Trialã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

        Returns:
            float: è©•ä¾¡çµæœã®å¹³å‡å ±é…¬
        """
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        config = {
            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),
            'n_steps': trial.suggest_categorical('n_steps', [1024, 2048, 4096]),
            'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),
            'n_epochs': trial.suggest_int('n_epochs', 5, 20),
            'gamma': trial.suggest_float('gamma', 0.9, 0.999),
            'gae_lambda': trial.suggest_float('gae_lambda', 0.9, 0.99),
            'clip_range': trial.suggest_float('clip_range', 0.1, 0.3),
            'ent_coef': trial.suggest_float('ent_coef', 0.0, 0.1),
            'vf_coef': trial.suggest_float('vf_coef', 0.3, 0.7),
            'total_timesteps': 50000,  # æœ€é©åŒ–æ™‚ã¯çŸ­ã‚ã«
            'eval_freq': 10000,
            'n_eval_episodes': 3,
            'log_dir': './logs/optuna/',
            'model_dir': './models/optuna/',
            'tensorboard_log': './tensorboard/optuna/',
            'verbose': 0,
            'seed': 42,
        }

        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
        trainer = PPOTrainer(data_path, config)
        model = trainer.train()

        # è©•ä¾¡
        eval_stats = trainer.evaluate(n_episodes=5)

        return eval_stats['mean_reward']

    # Optunaã‚¹ã‚¿ãƒ‡ã‚£ã®ä½œæˆ
    study = optuna.create_study(
        direction='maximize',
        sampler=TPESampler(),
        pruner=MedianPruner()
    )

    # æœ€é©åŒ–ã®å®Ÿè¡Œ
    study.optimize(objective, n_trials=n_trials)

    # æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¡¨ç¤º
    print("Best hyperparameters:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")

    print(f"Best reward: {study.best_value}")

    return study.best_params


def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°
    ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒ¢ãƒ¼ãƒ‰ã‚’æŒ‡å®šã—ã¦å®Ÿè¡Œ
    ä¾‹: python main.py --data ./data/train_features.parquet --mode train
    """
    import argparse

    parser = argparse.ArgumentParser(description='PPO Training for Heavy Trading Environment')
    parser.add_argument('--data', type=str, required=True, help='Path to training data (parquet or csv)')
    parser.add_argument('--mode', type=str, choices=['train', 'evaluate', 'optimize', 'visualize'],
                       default='train', help='Operation mode')
    parser.add_argument('--model-path', type=str, help='Path to model for evaluation')
    parser.add_argument('--n-trials', type=int, default=50, help='Number of optimization trials')
    parser.add_argument('--n-episodes', type=int, default=10, help='Number of evaluation episodes')

    args = parser.parse_args()

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆ
    trainer = PPOTrainer(args.data)

    if args.mode == 'train':
        # ã¾ãšãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿæ–½
        best_params = optimize_hyperparameters(args.data, args.n_trials)
        # total_timesteps ã‚’é™¤å¤–ã—ã¦ã‹ã‚‰ config ã‚’æ›´æ–°
        best_params.pop('total_timesteps', None)
        trainer.config.update(best_params)
        trainer.config['total_timesteps'] = 200000  # æœ¬ç•ªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
        # æœ¬ç•ªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½
        model = trainer.train()
        trainer.evaluate(n_episodes=args.n_episodes)
    elif args.mode == 'optimize':
        best_params = optimize_hyperparameters(args.data, args.n_trials)
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®æœ€çµ‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
        trainer.config.update(best_params)
        trainer.config['total_timesteps'] = 200000  # æœ¬ç•ªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
        model = trainer.train()

    elif args.mode == 'visualize':
        trainer.visualize_training()


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)